# 知识库管理系统 - 文件处理流程

## 整体流程

```
用户上传文件
    ↓
文件验证 (格式、大小、病毒扫描)
    ↓
保存到临时目录
    ↓
格式识别
    ↓
内容提取
    ↓
文本清洗
    ↓
文本分块
    ↓
向量化
    ↓
入库Milvus
    ↓
保存元数据到PostgreSQL
    ↓
返回成功响应
```

## 详细步骤

### 1. 文件验证

**验证项**:
- 文件格式是否支持
- 文件大小是否超限
- 文件是否损坏
- 文件是否包含恶意内容

**代码示例**:
```python
class FileValidator:
    ALLOWED_EXTENSIONS = {'.pdf', '.csv', '.txt', '.jpg', '.png', '.jpeg'}
    MAX_FILE_SIZE = 100 * 1024 * 1024  # 100MB
    
    def validate(self, file_path: str) -> bool:
        # 检查扩展名
        if not self._check_extension(file_path):
            raise ValueError("不支持的文件格式")
        
        # 检查文件大小
        if not self._check_size(file_path):
            raise ValueError("文件过大")
        
        # 检查文件完整性
        if not self._check_integrity(file_path):
            raise ValueError("文件损坏")
        
        return True
```

### 2. 内容提取

#### 2.1 PDF处理

```python
import PyPDF2

class PDFProcessor(FileProcessor):
    def process(self, file_path: str) -> str:
        text = []
        with open(file_path, 'rb') as f:
            reader = PyPDF2.PdfReader(f)
            for page in reader.pages:
                text.append(page.extract_text())
        return '\n'.join(text)
```

**特点**:
- 支持文本型PDF
- 支持扫描型PDF（需要OCR）
- 保留页码信息

#### 2.2 CSV处理

```python
import pandas as pd

class CSVProcessor(FileProcessor):
    def process(self, file_path: str) -> str:
        df = pd.read_csv(file_path)
        # 转换为文本格式
        text = df.to_string()
        return text
```

**特点**:
- 保留表头信息
- 支持多种编码
- 处理缺失值

#### 2.3 图片处理

```python
import pytesseract
from PIL import Image

class ImageProcessor(FileProcessor):
    def process(self, file_path: str) -> str:
        image = Image.open(file_path)
        # 图片预处理
        image = self._preprocess(image)
        # OCR识别
        text = pytesseract.image_to_string(image, lang='chi_sim+eng')
        return text
    
    def _preprocess(self, image: Image) -> Image:
        # 灰度化
        image = image.convert('L')
        # 二值化
        image = image.point(lambda x: 0 if x < 128 else 255, '1')
        return image
```

**特点**:
- 支持多种图片格式
- 支持多语言OCR
- 图片预处理提高准确率

#### 2.4 文本处理

```python
class TextProcessor(FileProcessor):
    def process(self, file_path: str) -> str:
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()
        return text
```

### 3. 文本清洗

```python
import re
from typing import List

class TextCleaner:
    def clean(self, text: str) -> str:
        # 移除特殊字符
        text = self._remove_special_chars(text)
        
        # 移除多余空白
        text = self._remove_extra_whitespace(text)
        
        # 统一编码
        text = self._normalize_encoding(text)
        
        # 移除重复行
        text = self._remove_duplicate_lines(text)
        
        return text
    
    def _remove_special_chars(self, text: str) -> str:
        # 保留中文、英文、数字和基本标点
        pattern = r'[^\u4e00-\u9fa5a-zA-Z0-9\s\.,，。！？；：]'
        return re.sub(pattern, '', text)
    
    def _remove_extra_whitespace(self, text: str) -> str:
        # 移除多余空白
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def _normalize_encoding(self, text: str) -> str:
        # 统一编码
        return text.encode('utf-8', 'ignore').decode('utf-8')
    
    def _remove_duplicate_lines(self, text: str) -> str:
        lines = text.split('\n')
        unique_lines = []
        for line in lines:
            if line not in unique_lines:
                unique_lines.append(line)
        return '\n'.join(unique_lines)
```

### 4. 文本分块

#### 4.1 固定长度分块

```python
class FixedChunker:
    def __init__(self, chunk_size: int = 512, overlap: int = 256):
        self.chunk_size = chunk_size
        self.overlap = overlap
    
    def chunk(self, text: str) -> List[str]:
        chunks = []
        start = 0
        while start < len(text):
            end = start + self.chunk_size
            chunks.append(text[start:end])
            start = end - self.overlap
        return chunks
```

#### 4.2 语义分块

```python
import nltk

class SemanticChunker:
    def __init__(self, max_chunk_size: int = 512):
        self.max_chunk_size = max_chunk_size
        nltk.download('punkt')
    
    def chunk(self, text: str) -> List[str]:
        # 按句子分割
        sentences = nltk.sent_tokenize(text)
        
        chunks = []
        current_chunk = []
        current_size = 0
        
        for sentence in sentences:
            sentence_size = len(sentence)
            
            if current_size + sentence_size > self.max_chunk_size:
                if current_chunk:
                    chunks.append(' '.join(current_chunk))
                current_chunk = [sentence]
                current_size = sentence_size
            else:
                current_chunk.append(sentence)
                current_size += sentence_size
        
        if current_chunk:
            chunks.append(' '.join(current_chunk))
        
        return chunks
```

### 5. 向量化

```python
from sentence_transformers import SentenceTransformer
import numpy as np

class Vectorizer:
    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
        self.model = SentenceTransformer(model_name)
    
    def vectorize(self, texts: List[str]) -> np.ndarray:
        """将文本转换为向量"""
        embeddings = self.model.encode(texts, show_progress_bar=True)
        return embeddings
    
    def vectorize_batch(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """批量向量化"""
        embeddings = self.model.encode(
            texts,
            batch_size=batch_size,
            show_progress_bar=True
        )
        return embeddings
```

### 6. 入库Milvus

```python
from pymilvus import Collection

class MilvusInserter:
    def __init__(self, collection_name: str):
        self.collection = Collection(collection_name)
    
    def insert(self, documents: List[dict]) -> List[int]:
        """插入文档到Milvus"""
        # 准备数据
        data = [
            [doc['id'] for doc in documents],
            [doc['content'] for doc in documents],
            [doc['embedding'] for doc in documents],
            [doc['metadata'] for doc in documents],
        ]
        
        # 插入数据
        mr = self.collection.insert(data)
        
        # 刷新数据
        self.collection.flush()
        
        return mr.primary_keys
```

### 7. 保存元数据

```python
from sqlalchemy.orm import Session
from app.models.database import Document

class MetadataStore:
    def save(self, session: Session, documents: List[dict]):
        """保存文档元数据到PostgreSQL"""
        for doc in documents:
            db_doc = Document(
                file_id=doc['file_id'],
                content=doc['content'],
                chunk_index=doc['chunk_index'],
                metadata=doc['metadata'],
            )
            session.add(db_doc)
        
        session.commit()
```

## 错误处理

```python
class FileProcessingError(Exception):
    """文件处理错误"""
    pass

class ProcessingPipeline:
    async def process_file(self, file_path: str) -> dict:
        try:
            # 验证
            self.validator.validate(file_path)
            
            # 提取内容
            content = self.processor.process(file_path)
            
            # 清洗
            content = self.cleaner.clean(content)
            
            # 分块
            chunks = self.chunker.chunk(content)
            
            # 向量化
            embeddings = self.vectorizer.vectorize(chunks)
            
            # 入库
            self.inserter.insert(embeddings)
            
            return {"status": "success", "chunks_count": len(chunks)}
        
        except FileNotFoundError as e:
            raise FileProcessingError(f"文件不存在: {e}")
        except ValueError as e:
            raise FileProcessingError(f"文件验证失败: {e}")
        except Exception as e:
            raise FileProcessingError(f"处理失败: {e}")
```

## 性能优化

### 1. 并行处理

```python
from concurrent.futures import ThreadPoolExecutor

class ParallelProcessor:
    def process_multiple(self, file_paths: List[str], max_workers: int = 4):
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            results = executor.map(self.process_file, file_paths)
        return list(results)
```

### 2. 批量操作

```python
# 批量向量化
embeddings = vectorizer.vectorize_batch(chunks, batch_size=32)

# 批量插入
inserter.insert_batch(documents, batch_size=1000)
```

### 3. 缓存

```python
from functools import lru_cache

@lru_cache(maxsize=128)
def get_processor(file_type: str):
    return PROCESSORS[file_type]
```

## 监控和日志

```python
import logging

logger = logging.getLogger(__name__)

class MonitoredProcessor:
    def process_file(self, file_path: str):
        logger.info(f"开始处理文件: {file_path}")
        
        try:
            result = self._process(file_path)
            logger.info(f"文件处理成功: {file_path}")
            return result
        except Exception as e:
            logger.error(f"文件处理失败: {file_path}, 错误: {e}")
            raise
```

