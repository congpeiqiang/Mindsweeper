('# Custom middleware - Docs by LangChain\n\n来源: https://docs.langchain.com/oss/python/langchain/middleware/custom\n\nCustom middleware - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...⌘KSupportGitHubTry LangSmithTry LangSmithSearch...NavigationMiddlewareCustom middlewareLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartChangelogPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingStructured outputMiddlewareOverviewBuilt-in middlewareCustom middlewareAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryAgent developmentLangSmith StudioTestAgent Chat UIDeploy with LangSmithDeploymentObservabilityOn this pageHooksNode-style hooksWrap-style hooksCreate middlewareDecorator-based middlewareClass-based middlewareCustom state schemaExecution orderAgent jumpsBest practicesExamplesDynamic model selectionTool call monitoringDynamically selecting toolsWorking with system messagesAdditional resourcesMiddlewareCustom middlewareCopy pageCopy pageBuild custom middleware by implementing hooks that run at specific points in the agent execution flow.\n\u200bHooks\nMiddleware provides two styles of hooks to intercept agent execution:\nNode-style hooksRun sequentially at specific execution points.Wrap-style hooksRun around each model or tool call.\n\u200bNode-style hooks\nRun sequentially at specific execution points. Use for logging, validation, and state updates.\nAvailable hooks:\nbefore_agent - Before agent starts (once per invocation)\nbefore_model - Before each model call\nafter_model - After each model response\nafter_agent - After agent completes (once per invocation)\nExample:\nDecorator ClassCopyfrom langchain.agents.middleware import before_model, after_model, AgentState\nfrom langchain.messages import AIMessage\nfrom langgraph.runtime import Runtime\nfrom typing import Any\n@before_model(can_jump_to=["end"])\ndef check_message_limit(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\nif len(state["messages"]) >= 50:\nreturn {\n"messages": [AIMessage("Conversation limit reached.")],\n"jump_to": "end"\n}\nreturn None\n@after_model\ndef log_response(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\nprint(f"Model returned: {state[\'messages\'][-1].content}")\nreturn None\nCopyfrom langchain.agents.middleware import AgentMiddleware, AgentState, hook_config\nfrom langchain.messages import AIMessage\nfrom langgraph.runtime import Runtime\nfrom typing import Any\nclass MessageLimitMiddleware(AgentMiddleware):\ndef __init__(self, max_messages: int = 50):\nsuper().__init__()\nself.max_messages = max_messages\n@hook_config(can_jump_to=["end"])\ndef before_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\nif len(state["messages"]) == self.max_messages:\nreturn {\n"messages": [AIMessage("Conversation limit reached.")],\n"jump_to": "end"\n}\nreturn None\ndef after_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\nprint(f"Model returned: {state[\'messages\'][-1].content}")\nreturn None\n\u200bWrap-style hooks\nIntercept execution and control when the handler is called. Use for retries, caching, and transformation.\nYou decide if the handler is called zero times (short-circuit), once (normal flow), or multiple times (retry logic).\nAvailable hooks:\nwrap_model_call - Around each model call\nwrap_tool_call - Around each tool call\nExample:\nDecorator ClassCopyfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\nfrom typing import Callable\n@wrap_model_call\ndef retry_model(\nrequest: ModelRequest,\nhandler: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\nfor attempt in range(3):\ntry:\nreturn handler(request)\nexcept Exception as e:\nif attempt == 2:\nraise\nprint(f"Retry {attempt + 1}/3 after error: {e}")\nCopyfrom langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse\nfrom typing import Callable\nclass RetryMiddleware(AgentMiddleware):\ndef __init__(self, max_retries: int = 3):\nsuper().__init__()\nself.max_retries = max_retries\ndef wrap_model_call(\nself,\nrequest: ModelRequest,\nhandler: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\nfor attempt in range(self.max_retries):\ntry:\nreturn handler(request)\nexcept Exception as e:\nif attempt == self.max_retries - 1:\nraise\nprint(f"Retry {attempt + 1}/{self.max_retries} after error: {e}")\n\u200bCreate middleware\nYou can create middleware in two ways:\nDecorator-based middlewareQuick and simple for single-hook middleware. Use decorators to wrap individual functions.Class-based middlewareMore powerful for complex middleware with multiple hooks or configuration.\n\u200bDecorator-based middleware\nQuick and simple for single-hook middleware. Use decorators to wrap individual functions.\nAvailable decorators:\nNode-style:\n@before_agent - Runs before agent starts (once per invocation)\n@before_model - Runs before each model call\n@after_model - Runs after each model response\n@after_agent - Runs after agent completes (once per invocation)\nWrap-style:\n@wrap_model_call - Wraps each model call with custom logic\n@wrap_tool_call - Wraps each tool call with custom logic\nConvenience:\n@dynamic_prompt - Generates dynamic system prompts\nExample:\nCopyfrom langchain.agents.middleware import (\nbefore_model,\nwrap_model_call,\nAgentState,\nModelRequest,\nModelResponse,\n)\nfrom langchain.agents import create_agent\nfrom langgraph.runtime import Runtime\nfrom typing import Any, Callable\n@before_model\ndef log_before_model(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\nprint(f"About to call model with {len(state[\'messages\'])} messages")\nreturn None\n@wrap_model_call\ndef retry_model(\nrequest: ModelRequest,\nhandler: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\nfor attempt in range(3):\ntry:\nreturn handler(request)\nexcept Exception as e:\nif attempt == 2:\nraise\nprint(f"Retry {attempt + 1}/3 after error: {e}")\nagent = create_agent(\nmodel="gpt-4o",\nmiddleware=[log_before_model, retry_model],\ntools=[...],\n)\nWhen to use decorators:\nSingle hook needed\nNo complex configuration\nQuick prototyping\n\u200bClass-based middleware\nMore powerful for complex middleware with multiple hooks or configuration. Use classes when you need to define both sync and async implementations for the same hook, or when you want to combine multiple hooks in a single middleware.\nExample:\nCopyfrom langchain.agents.middleware import (\nAgentMiddleware,\nAgentState,\nModelRequest,\nModelResponse,\n)\nfrom langgraph.runtime import Runtime\nfrom typing import Any, Callable\nclass LoggingMiddleware(AgentMiddleware):\ndef before_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\nprint(f"About to call model with {len(state[\'messages\'])} messages")\nreturn None\ndef after_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\nprint(f"Model returned: {state[\'messages\'][-1].content}")\nreturn None\nagent = create_agent(\nmodel="gpt-4o",\nmiddleware=[LoggingMiddleware()],\ntools=[...],\n)\nWhen to use classes:\nDefining both sync and async implementations for the same hook\nMultiple hooks needed in a single middleware\nComplex configuration required (e.g., configurable thresholds, custom models)\nReuse across projects with init-time configuration\n\u200bCustom state schema\nMiddleware can extend the agent’s state with custom properties. This enables middleware to:\nTrack state across execution: Maintain counters, flags, or other values that persist throughout the agent’s execution lifecycle\nShare data between hooks: Pass information from before_model to after_model or between different middleware instances\nImplement cross-cutting concerns: Add functionality like rate limiting, usage tracking, user context, or audit logging without modifying the core agent logic\nMake conditional decisions: Use accumulated state to determine whether to continue execution, jump to different nodes, or modify behavior dynamically\nDecorator ClassCopyfrom langchain.agents import create_agent\nfrom langchain.messages import HumanMessage\nfrom langchain.agents.middleware import AgentState, before_model, after_model\nfrom typing_extensions import NotRequired\nfrom typing import Any\nfrom langgraph.runtime import Runtime\nclass CustomState(AgentState):\nmodel_call_count: NotRequired[int]\nuser_id: NotRequired[str]\n@before_model(state_schema=CustomState, can_jump_to=["end"])\ndef check_call_limit(state: CustomState, runtime: Runtime) -> dict[str, Any] | None:\ncount = state.get("model_call_count", 0)\nif count > 10:\nreturn {"jump_to": "end"}\nreturn None\n@after_model(state_schema=CustomState)\ndef increment_counter(state: CustomState, runtime: Runtime) -> dict[str, Any] | None:\nreturn {"model_call_count": state.get("model_call_count", 0) + 1}\nagent = create_agent(\nmodel="gpt-4o",\nmiddleware=[check_call_limit, increment_counter],\ntools=[],\n)\n# Invoke with custom state\nresult = agent.invoke({\n"messages": [HumanMessage("Hello")],\n"model_call_count": 0,\n"user_id": "user-123",\n})\nCopyfrom langchain.agents import create_agent\nfrom langchain.messages import HumanMessage\nfrom langchain.agents.middleware import AgentState, AgentMiddleware\nfrom typing_extensions import NotRequired\nfrom typing import Any\nclass CustomState(AgentState):\nmodel_call_count: NotRequired[int]\nuser_id: NotRequired[str]\nclass CallCounterMiddleware(AgentMiddleware[CustomState]):\nstate_schema = CustomState\ndef before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\ncount = state.get("model_call_count", 0)\nif count > 10:\nreturn {"jump_to": "end"}\nreturn None\ndef after_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\nreturn {"model_call_count": state.get("model_call_count", 0) + 1}\nagent = create_agent(\nmodel="gpt-4o",\nmiddleware=[CallCounterMiddleware()],\ntools=[],\n)\n# Invoke with custom state\nresult = agent.invoke({\n"messages": [HumanMessage("Hello")],\n"model_call_count": 0,\n"user_id": "user-123",\n})\n\u200bExecution order\nWhen using multiple middleware, understand how they execute:\nCopyagent = create_agent(\nmodel="gpt-4o",\nmiddleware=[middleware1, middleware2, middleware3],\ntools=[...],\n)\nExecution flowBefore hooks run in order:\nmiddleware1.before_agent()\nmiddleware2.before_agent()\nmiddleware3.before_agent()\nAgent loop starts\nmiddleware1.before_model()\nmiddleware2.before_model()\nmiddleware3.before_model()\nWrap hooks nest like function calls:\nmiddleware1.wrap_model_call() → middleware2.wrap_model_call() → middleware3.wrap_model_call() → model\nAfter hooks run in reverse order:\nmiddleware3.after_model()\nmiddleware2.after_model()\nmiddleware1.after_model()\nAgent loop ends\nmiddleware3.after_agent()\nmiddleware2.after_agent()\nmiddleware1.after_agent()\nKey rules:\nbefore_* hooks: First to last\nafter_* hooks: Last to first (reverse)\nwrap_* hooks: Nested (first middleware wraps all others)\n\u200bAgent jumps\nTo exit early from middleware, return a dictionary with jump_to:\nAvailable jump targets:\n\'end\': Jump to the end of the agent execution (or the first after_agent hook)\n\'tools\': Jump to the tools node\n\'model\': Jump to the model node (or the first before_model hook)\nDecorator ClassCopyfrom langchain.agents.middleware import after_model, hook_config, AgentState\nfrom langchain.messages import AIMessage\nfrom langgraph.runtime import Runtime\nfrom typing import Any\n@after_model\n@hook_config(can_jump_to=["end"])\ndef check_for_blocked(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\nlast_message = state["messages"][-1]\nif "BLOCKED" in last_message.content:\nreturn {\n"messages": [AIMessage("I cannot respond to that request.")],\n"jump_to": "end"\n}\nreturn None\nCopyfrom langchain.agents.middleware import AgentMiddleware, hook_config, AgentState\nfrom langchain.messages import AIMessage\nfrom langgraph.runtime import Runtime\nfrom typing import Any\nclass BlockedContentMiddleware(AgentMiddleware):\n@hook_config(can_jump_to=["end"])\ndef after_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\nlast_message = state["messages"][-1]\nif "BLOCKED" in last_message.content:\nreturn {\n"messages": [AIMessage("I cannot respond to that request.")],\n"jump_to": "end"\n}\nreturn None\n\u200bBest practices\nKeep middleware focused - each should do one thing well\nHandle errors gracefully - don’t let middleware errors crash the agent\nUse appropriate hook types:\nNode-style for sequential logic (logging, validation)\nWrap-style for control flow (retry, fallback, caching)\nClearly document any custom state properties\nUnit test middleware independently before integrating\nConsider execution order - place critical middleware first in the list\nUse built-in middleware when possible\n\u200bExamples\n\u200bDynamic model selection\nDecorator ClassCopyfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\nfrom langchain.chat_models import init_chat_model\nfrom typing import Callable\ncomplex_model = init_chat_model("gpt-4o")\nsimple_model = init_chat_model("gpt-4o-mini")\n@wrap_model_call\ndef dynamic_model(\nrequest: ModelRequest,\nhandler: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n# Use different model based on conversation length\nif len(request.messages) > 10:\nmodel = complex_model\nelse:\nmodel = simple_model\nreturn handler(request.override(model=model))\nCopyfrom langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse\nfrom langchain.chat_models import init_chat_model\nfrom typing import Callable\ncomplex_model = init_chat_model("gpt-4o")\nsimple_model = init_chat_model("gpt-4o-mini")\nclass DynamicModelMiddleware(AgentMiddleware):\ndef wrap_model_call(\nself,\nrequest: ModelRequest,\nhandler: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n# Use different model based on conversation length\nif len(request.messages) > 10:\nmodel = complex_model\nelse:\nmodel = simple_model\nreturn handler(request.override(model=model))\n\u200bTool call monitoring\nDecorator ClassCopyfrom langchain.agents.middleware import wrap_tool_call\nfrom langchain.tools.tool_node import ToolCallRequest\nfrom langchain.messages import ToolMessage\nfrom langgraph.types import Command\nfrom typing import Callable\n@wrap_tool_call\ndef monitor_tool(\nrequest: ToolCallRequest,\nhandler: Callable[[ToolCallRequest], ToolMessage | Command],\n) -> ToolMessage | Command:\nprint(f"Executing tool: {request.tool_call[\'name\']}")\nprint(f"Arguments: {request.tool_call[\'args\']}")\ntry:\nresult = handler(request)\nprint(f"Tool completed successfully")\nreturn result\nexcept Exception as e:\nprint(f"Tool failed: {e}")\nraise\nCopyfrom langchain.tools.tool_node import ToolCallRequest\nfrom langchain.agents.middleware import AgentMiddleware\nfrom langchain.messages import ToolMessage\nfrom langgraph.types import Command\nfrom typing import Callable\nclass ToolMonitoringMiddleware(AgentMiddleware):\ndef wrap_tool_call(\nself,\nrequest: ToolCallRequest,\nhandler: Callable[[ToolCallRequest], ToolMessage | Command],\n) -> ToolMessage | Command:\nprint(f"Executing tool: {request.tool_call[\'name\']}")\nprint(f"Arguments: {request.tool_call[\'args\']}")\ntry:\nresult = handler(request)\nprint(f"Tool completed successfully")\nreturn result\nexcept Exception as e:\nprint(f"Tool failed: {e}")\nraise\n\u200bDynamically selecting tools\nSelect relevant tools at runtime to improve performance and accuracy.\nBenefits:\nShorter prompts - Reduce complexity by exposing only relevant tools\nBetter accuracy - Models choose correctly from fewer options\nPermission control - Dynamically filter tools based on user access\nDecorator ClassCopyfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\nfrom typing import Callable\n@wrap_model_call\ndef select_tools(\nrequest: ModelRequest,\nhandler: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n"""Middleware to select relevant tools based on state/context."""\n# Select a small, relevant subset of tools based on state/context\nrelevant_tools = select_relevant_tools(request.state, request.runtime)\nreturn handler(request.override(tools=relevant_tools))\nagent = create_agent(\nmodel="gpt-4o",\ntools=all_tools,\n# All available tools need to be registered upfront\nmiddleware=[select_tools],\n)\nCopyfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse\nfrom typing import Callable\nclass ToolSelectorMiddleware(AgentMiddleware):\ndef wrap_model_call(\nself,\nrequest: ModelRequest,\nhandler: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n"""Middleware to select relevant tools based on state/context."""\n# Select a small, relevant subset of tools based on state/context\nrelevant_tools = select_relevant_tools(request.state, request.runtime)\nreturn handler(request.override(tools=relevant_tools))\nagent = create_agent(\nmodel="gpt-4o",\ntools=all_tools,\n# All available tools need to be registered upfront\nmiddleware=[ToolSelectorMiddleware()],\n)\n\u200bWorking with system messages\nModify system messages in middleware using the system_message field on ModelRequest. The system_message field contains a SystemMessage object (even if the agent was created with a string system_prompt).\nExample: Adding context to system message\nDecorator ClassCopyfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\nfrom langchain.messages import SystemMessage\nfrom typing import Callable\n@wrap_model_call\ndef add_context(\nrequest: ModelRequest,\nhandler: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n# Always work with content blocks\nnew_content = list(request.system_message.content_blocks) + [\n{"type": "text", "text": "Additional context."}\n]\nnew_system_message = SystemMessage(content=new_content)\nreturn handler(request.override(system_message=new_system_message))\nCopyfrom langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse\nfrom langchain.messages import SystemMessage\nfrom typing import Callable\nclass ContextMiddleware(AgentMiddleware):\ndef wrap_model_call(\nself,\nrequest: ModelRequest,\nhandler: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n# Always work with content blocks\nnew_content = list(request.system_message.content_blocks) + [\n{"type": "text", "text": "Additional context."}\n]\nnew_system_message = SystemMessage(content=new_content)\nreturn handler(request.override(system_message=new_system_message))\nExample: Working with cache control (Anthropic)\nWhen working with Anthropic models, you can use structured content blocks with cache control directives to cache large system prompts:\nDecorator ClassCopyfrom langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\nfrom langchain.messages import SystemMessage\nfrom typing import Callable\n@wrap_model_call\ndef add_cached_context(\nrequest: ModelRequest,\nhandler: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n# Always work with content blocks\nnew_content = list(request.system_message.content_blocks) + [\n{\n"type": "text",\n"text": "Here is a large document to analyze:\\n\\n<document>...</document>",\n# content up until this point is cached\n"cache_control": {"type": "ephemeral"}\n}\n]\nnew_system_message = SystemMessage(content=new_content)\nreturn handler(request.override(system_message=new_system_message))\nCopyfrom langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse\nfrom langchain.messages import SystemMessage\nfrom typing import Callable\nclass CachedContextMiddleware(AgentMiddleware):\ndef wrap_model_call(\nself,\nrequest: ModelRequest,\nhandler: Callable[[ModelRequest], ModelResponse],\n) -> ModelResponse:\n# Always work with content blocks\nnew_content = list(request.system_message.content_blocks) + [\n{\n"type": "text",\n"text": "Here is a large document to analyze:\\n\\n<document>...</document>",\n"cache_control": {"type": "ephemeral"}\n# This content will be cached\n}\n]\nnew_system_message = SystemMessage(content=new_content)\nreturn handler(request.override(system_message=new_system_message))\nNotes:\nModelRequest.system_message is always a SystemMessage object, even if the agent was created with system_prompt="string"\nUse SystemMessage.content_blocks to access content as a list of blocks, regardless of whether the original content was a string or list\nWhen modifying system messages, use content_blocks and append new blocks to preserve existing structure\nYou can pass SystemMessage objects directly to create_agent’s system_prompt parameter for advanced use cases like cache control\n\u200bAdditional resources\nMiddleware API reference\nBuilt-in middleware\nTesting agents\nEdit this page on GitHub or file an issue.\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoBuilt-in middlewarePreviousGuardrailsNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by', 'Custom middleware - Docs by LangChain', 'docs.langchain.com')
